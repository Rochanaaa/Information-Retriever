{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7672661,"sourceType":"datasetVersion","datasetId":4475433},{"sourceId":815876,"sourceType":"datasetVersion","datasetId":429085}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-04T16:08:21.061560Z","iopub.execute_input":"2024-03-04T16:08:21.062329Z","iopub.status.idle":"2024-03-04T16:08:21.066259Z","shell.execute_reply.started":"2024-03-04T16:08:21.062289Z","shell.execute_reply":"2024-03-04T16:08:21.065344Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Set up and import necessary packages","metadata":{}},{"cell_type":"code","source":"%pip install xlrd","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:08:23.080867Z","iopub.execute_input":"2024-03-04T16:08:23.081547Z","iopub.status.idle":"2024-03-04T16:08:39.752072Z","shell.execute_reply.started":"2024-03-04T16:08:23.081517Z","shell.execute_reply":"2024-03-04T16:08:39.750886Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting xlrd\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\nDownloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: xlrd\nSuccessfully installed xlrd-2.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\n\nfrom collections import Counter\nimport string\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport re\nimport subprocess\nimport tqdm\nimport time\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nnltk.download('wordnet')\nnltk.download('stopwords')        ","metadata":{"execution":{"iopub.status.busy":"2024-03-04T15:54:16.091700Z","iopub.execute_input":"2024-03-04T15:54:16.091978Z","iopub.status.idle":"2024-03-04T15:54:16.318915Z","shell.execute_reply.started":"2024-03-04T15:54:16.091954Z","shell.execute_reply":"2024-03-04T15:54:16.318040Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Download and unzip wordnet (for kaggle only)\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T15:54:16.320616Z","iopub.execute_input":"2024-03-04T15:54:16.321226Z","iopub.status.idle":"2024-03-04T15:54:16.365828Z","shell.execute_reply.started":"2024-03-04T15:54:16.321192Z","shell.execute_reply":"2024-03-04T15:54:16.364970Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /kaggle/working/...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /kaggle/working/corpora/wordnet.zip\n","output_type":"stream"},{"name":"stderr","text":"replace /kaggle/working/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n(EOF or read error, treating as \"[N]one\" ...)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load and clean dataset","metadata":{}},{"cell_type":"code","source":"# Load reddit dataset\n# data = pd.read_csv('/kaggle/input/twitter-and-reddit-sentimental-analysis-dataset/Reddit_Data.csv',\n#                    names=['review', 'sentiment']).drop(0)\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:10:42.846324Z","iopub.execute_input":"2024-03-04T16:10:42.846915Z","iopub.status.idle":"2024-03-04T16:10:42.979207Z","shell.execute_reply.started":"2024-03-04T16:10:42.846870Z","shell.execute_reply":"2024-03-04T16:10:42.978134Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n1   family mormon have never tried explain them t...         1\n2  buddhism has very much lot compatible with chr...         1\n3  seriously don say thing first all they won get...        -1\n4  what you have learned yours and only yours wha...         0\n5  for your own benefit you may want read living ...         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>family mormon have never tried explain them t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>buddhism has very much lot compatible with chr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>seriously don say thing first all they won get...</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>what you have learned yours and only yours wha...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>for your own benefit you may want read living ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load dataset\ndata = pd.read_excel('/kaggle/input/sentiment-analysis/datasets.xls',\n                     sheet_name='IMDb',\n                     header=None,\n                     names=['review', 'sentiment']\n)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:11:04.738092Z","iopub.execute_input":"2024-03-04T16:11:04.739123Z","iopub.status.idle":"2024-03-04T16:11:07.978791Z","shell.execute_reply.started":"2024-03-04T16:11:04.739085Z","shell.execute_reply":"2024-03-04T16:11:07.977876Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  POSITIVE\n1  A wonderful little production. <br /><br />The...  POSITIVE\n2  I thought this was a wonderful way to spend ti...  POSITIVE\n3  Basically there's a family where a little boy ...  NEGATIVE\n4  Petter Mattei's \"Love in the Time of Money\" is...  POSITIVE","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>POSITIVE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Split data into train and test sets","metadata":{}},{"cell_type":"code","source":"def split_data(data_df, test_size, seed=None):\n    train_df, validate_test_df = train_test_split(data_df, test_size=test_size, random_state=seed)\n    validate_df, test_df = train_test_split(validate_test_df, test_size=0.5, random_state=seed)\n    return train_df, validate_df, test_df","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:11:14.071672Z","iopub.execute_input":"2024-03-04T16:11:14.072521Z","iopub.status.idle":"2024-03-04T16:11:14.079958Z","shell.execute_reply.started":"2024-03-04T16:11:14.072478Z","shell.execute_reply":"2024-03-04T16:11:14.078751Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"seed = 11\ntrain_df, validate_df, test_df = split_data(data, 0.2, seed)\ntrain_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:11:14.369349Z","iopub.execute_input":"2024-03-04T16:11:14.371469Z","iopub.status.idle":"2024-03-04T16:11:14.463097Z","shell.execute_reply.started":"2024-03-04T16:11:14.371428Z","shell.execute_reply":"2024-03-04T16:11:14.462022Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                                   review sentiment\ncount                                               40000     40000\nunique                                              39738         2\ntop     Loved today's show!!! It was a variety and not...  NEGATIVE\nfreq                                                    5     20046","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>40000</td>\n      <td>40000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>39738</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Loved today's show!!! It was a variety and not...</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>5</td>\n      <td>20046</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:11:14.713761Z","iopub.execute_input":"2024-03-04T16:11:14.714228Z","iopub.status.idle":"2024-03-04T16:11:14.727071Z","shell.execute_reply.started":"2024-03-04T16:11:14.714189Z","shell.execute_reply":"2024-03-04T16:11:14.725839Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment\n10024  Painful. Painful is the only word to describe ...  NEGATIVE\n11762  This is a cute little French silent comedy abo...  POSITIVE\n43345  I loved this show when it aired on television ...  POSITIVE\n33685  It pains me to write such a scathing review bu...  NEGATIVE\n23647  \"Stairway to Heaven\" is a outstanding inventio...  POSITIVE","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10024</th>\n      <td>Painful. Painful is the only word to describe ...</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>11762</th>\n      <td>This is a cute little French silent comedy abo...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>43345</th>\n      <td>I loved this show when it aired on television ...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>33685</th>\n      <td>It pains me to write such a scathing review bu...</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>23647</th>\n      <td>\"Stairway to Heaven\" is a outstanding inventio...</td>\n      <td>POSITIVE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"validate_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:11:15.064250Z","iopub.execute_input":"2024-03-04T16:11:15.065253Z","iopub.status.idle":"2024-03-04T16:11:15.088161Z","shell.execute_reply.started":"2024-03-04T16:11:15.065198Z","shell.execute_reply":"2024-03-04T16:11:15.087237Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                                   review sentiment\ncount                                                5000      5000\nunique                                               4999         2\ntop     Well I guess I know the answer to that questio...  NEGATIVE\nfreq                                                    2      2522","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5000</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>4999</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Well I guess I know the answer to that questio...</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>2</td>\n      <td>2522</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:11:15.358415Z","iopub.execute_input":"2024-03-04T16:11:15.359288Z","iopub.status.idle":"2024-03-04T16:11:15.385674Z","shell.execute_reply.started":"2024-03-04T16:11:15.359244Z","shell.execute_reply":"2024-03-04T16:11:15.384529Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                                                   review sentiment\ncount                                                5000      5000\nunique                                               4998         2\ntop     Bette Midler is again Divine! Raunchily humoro...  POSITIVE\nfreq                                                    2      2568","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5000</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>4998</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Bette Midler is again Divine! Raunchily humoro...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>2</td>\n      <td>2568</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Convert text to word embeddings","metadata":{}},{"cell_type":"code","source":"def preprocess(text):\n    # Clean dataset i.e. remove <br /> and punctuations\n    text = text.lower()\n    text = re.sub('<br />', '', text)\n    text = re.sub('https://.*','', text)   #remove URLs\n    text = re.sub('[^a-zA-Z0-9 ]', '', text)    #remove non-alphanumeric characters \n\n    # Tokenise and get POS tags of each word\n    tokens = word_tokenize(text)\n    tags = nltk.pos_tag(tokens)\n\n    # Lemmatization\n    lem = WordNetLemmatizer()\n    lemmatized = []\n    for token, tag in tags:\n        pos = tag[0].lower() if tag[0].lower() in ['a', 'r', 'n', 'v'] else 'n'\n        lemmatized_token = lem.lemmatize(token, pos=pos)\n        lemmatized.append(lemmatized_token)\n        \n    # Remove stop words\n    stop_words = list(stopwords.words('english'))\n    filtered_tokens = [token for token in lemmatized if token.lower() not in stop_words]\n    \n    return filtered_tokens\n\n# Convert reviews to features\ndef encode_reviews(sentence, vocab):\n    features = []\n    for word in sentence:\n        if word not in vocab.keys():\n            features.append(0)\n        else:\n            features.append(vocab[word])\n    features = np.array(features)\n    return features\n\n# Pads data to max length\ndef padding(sentence, max_seq_len):\n    \"\"\"\n    Adds padding if sentence length is too short, else truncate sentence to a fixed length.\n    Args:\n        sentence (np.array): list of preprocessed words in a single review\n        max_seq_len (int): max length of a sequence\n    Returns:\n        list: padded or truncated list of words\n    \"\"\"\n    \n    # Add padding to sentence\n    if len(sentence) < max_seq_len:\n        num_padding = max_seq_len - len(sentence)\n        padding = np.zeros(num_padding, dtype=int)\n        sentence = np.concatenate((sentence, padding))\n    \n    # Truncate sentence\n    else:\n        sentence = sentence[:max_seq_len]\n            \n    return sentence","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:11:15.788658Z","iopub.execute_input":"2024-03-04T16:11:15.789095Z","iopub.status.idle":"2024-03-04T16:11:15.804878Z","shell.execute_reply.started":"2024-03-04T16:11:15.789060Z","shell.execute_reply":"2024-03-04T16:11:15.803761Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(data, max_seq_len, vocab):\n    \"\"\"\n    Preprocess data, including tokenisation, lemmatisation, encoding and padding\n    Args:\n        data (DataFrame): dataframe consisting of x (review) and y (sentiment) data\n        max_seq_len (int): max length of x data\n        vocab (dict): list of words ordered by descending num of occurences\n    Returns:\n        numpy_array: preprocessed reviews\n        numpy_array: preprocessed sentiments\n    \"\"\"\n    \n    data_copy = data.copy()\n\n    # Process review\n    data_copy['review'] = data_copy['review'].apply(encode_reviews, vocab=vocab)\n    x = data_copy['review'].apply(padding, max_seq_len=max_seq_len).to_numpy()\n    x = np.vstack(x)\n#     x.astype(int)\n    \n    # Process sentiment\n    # TODO: one-hot encode this\n    y = np.array([1 if label.strip()=='POSITIVE' else 0 for label in data_copy['sentiment']])\n    \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:11:16.024769Z","iopub.execute_input":"2024-03-04T16:11:16.025456Z","iopub.status.idle":"2024-03-04T16:11:16.033741Z","shell.execute_reply.started":"2024-03-04T16:11:16.025417Z","shell.execute_reply":"2024-03-04T16:11:16.032655Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def create_dict(data):\n    # Create dictionary\n    word_list = []\n    word_list = data.explode().reset_index(drop=True)\n\n    # Count all the words using Counter Method\n    count_words = Counter(word_list)\n    sorted_words = count_words.most_common(len(word_list))\n    \n    # Add unknown (0) into dictionary\n    vocab_to_int = {w:i+1 for i,(w,c) in enumerate(sorted_words)}\n\n    return vocab_to_int","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:11:16.315101Z","iopub.execute_input":"2024-03-04T16:11:16.315872Z","iopub.status.idle":"2024-03-04T16:11:16.321999Z","shell.execute_reply.started":"2024-03-04T16:11:16.315835Z","shell.execute_reply":"2024-03-04T16:11:16.321142Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Longest data processing step\nstart_time = time.time()\n\ntrain_df['review'] = train_df['review'].apply(preprocess)\ntime_taken = time.time() - start_time\n\nprint(f\"time taken: {time_taken/60:.2f}min\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:11:16.507603Z","iopub.execute_input":"2024-03-04T16:11:16.507898Z","iopub.status.idle":"2024-03-04T16:22:26.698968Z","shell.execute_reply.started":"2024-03-04T16:11:16.507874Z","shell.execute_reply":"2024-03-04T16:22:26.697856Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"time taken: 11.17min\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:22:26.701246Z","iopub.execute_input":"2024-03-04T16:22:26.701689Z","iopub.status.idle":"2024-03-04T16:22:26.719754Z","shell.execute_reply.started":"2024-03-04T16:22:26.701651Z","shell.execute_reply":"2024-03-04T16:22:26.718869Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment\n10024  [painful, painful, word, describe, awful, rend...  NEGATIVE\n11762  [cute, little, french, silent, comedy, man, be...  POSITIVE\n43345  [love, show, air, television, crush, find, som...  POSITIVE\n33685  [pain, write, scathing, review, im, simply, en...  NEGATIVE\n23647  [stairway, heaven, outstanding, invention, mov...  POSITIVE","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10024</th>\n      <td>[painful, painful, word, describe, awful, rend...</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>11762</th>\n      <td>[cute, little, french, silent, comedy, man, be...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>43345</th>\n      <td>[love, show, air, television, crush, find, som...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>33685</th>\n      <td>[pain, write, scathing, review, im, simply, en...</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>23647</th>\n      <td>[stairway, heaven, outstanding, invention, mov...</td>\n      <td>POSITIVE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Get vocab from training data\nvocab = create_dict(train_df['review'])\nlist(vocab.items())[:5]","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:22:26.720957Z","iopub.execute_input":"2024-03-04T16:22:26.721271Z","iopub.status.idle":"2024-03-04T16:22:28.490881Z","shell.execute_reply.started":"2024-03-04T16:22:26.721246Z","shell.execute_reply":"2024-03-04T16:22:28.489727Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[('movie', 1), ('film', 2), ('one', 3), ('make', 4), ('like', 5)]"},"metadata":{}}]},{"cell_type":"code","source":"# Determine value of max_seq_len\nreview_len = pd.DataFrame(train_df['review'].apply(len))\nreview_len.hist()\nplt.show()\nreview_len.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:22:28.493142Z","iopub.execute_input":"2024-03-04T16:22:28.493509Z","iopub.status.idle":"2024-03-04T16:22:28.909054Z","shell.execute_reply.started":"2024-03-04T16:22:28.493478Z","shell.execute_reply":"2024-03-04T16:22:28.908027Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"             review\ncount  40000.000000\nmean     119.462525\nstd       90.111400\nmin        4.000000\n25%       64.000000\n50%       89.000000\n75%      145.000000\nmax     1426.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>40000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>119.462525</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>90.111400</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>64.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>89.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>145.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1426.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"max_seq_len = 250","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:22:28.910671Z","iopub.execute_input":"2024-03-04T16:22:28.911069Z","iopub.status.idle":"2024-03-04T16:22:28.915944Z","shell.execute_reply.started":"2024-03-04T16:22:28.911038Z","shell.execute_reply":"2024-03-04T16:22:28.914830Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Preprocess data\nstart_time = time.time()\n\ntrain_x, train_y = preprocess_data(train_df, max_seq_len, vocab)\ntime_taken = time.time() - start_time\n\nprint(f\"time taken: {time_taken:.2f}s\")\nprint(f\"Shape of x: {train_x.shape}, shape of y: {train_y.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:22:28.917111Z","iopub.execute_input":"2024-03-04T16:22:28.917513Z","iopub.status.idle":"2024-03-04T16:22:31.925410Z","shell.execute_reply.started":"2024-03-04T16:22:28.917468Z","shell.execute_reply":"2024-03-04T16:22:31.924443Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"time taken: 3.00s\nShape of x: (40000, 250), shape of y: (40000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Process validation dataset\nstart_time = time.time()\nvalidate_df['review'] = validate_df['review'].apply(preprocess)\ntime_taken = time.time() - start_time\n\nprint(f\"time taken: {time_taken:.2f}s\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:22:31.926542Z","iopub.execute_input":"2024-03-04T16:22:31.926838Z","iopub.status.idle":"2024-03-04T16:23:55.514004Z","shell.execute_reply.started":"2024-03-04T16:22:31.926815Z","shell.execute_reply":"2024-03-04T16:23:55.512897Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"time taken: 83.58s\n","output_type":"stream"}]},{"cell_type":"code","source":"start_time = time.time()\nvalidate_x, validate_y = preprocess_data(validate_df, max_seq_len, vocab)\ntime_taken = time.time() - start_time\n\nprint(f\"time taken: {time_taken:.2f}s\")\nprint(f\"Shape of x: {validate_x.shape}, shape of y: {validate_y.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:23:55.515562Z","iopub.execute_input":"2024-03-04T16:23:55.515969Z","iopub.status.idle":"2024-03-04T16:23:55.915741Z","shell.execute_reply.started":"2024-03-04T16:23:55.515933Z","shell.execute_reply":"2024-03-04T16:23:55.914785Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"time taken: 0.39s\nShape of x: (5000, 250), shape of y: (5000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Process test dataset\ntest_df['review'] = test_df['review'].apply(preprocess)\ntime_taken = time.time() - start_time\n\nprint(f\"time taken: {time_taken:.2f}s\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:23:55.916932Z","iopub.execute_input":"2024-03-04T16:23:55.917216Z","iopub.status.idle":"2024-03-04T16:25:19.554450Z","shell.execute_reply.started":"2024-03-04T16:23:55.917192Z","shell.execute_reply":"2024-03-04T16:25:19.553506Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"time taken: 84.03s\n","output_type":"stream"}]},{"cell_type":"code","source":"start_time = time.time()\ntest_x, test_y = preprocess_data(test_df, max_seq_len, vocab)\ntime_taken = time.time() - start_time\n\nprint(f\"time taken: {time_taken:.2f}s\")\nprint(f\"Shape of x: {test_x.shape}, shape of y: {test_y.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:25:19.558418Z","iopub.execute_input":"2024-03-04T16:25:19.558991Z","iopub.status.idle":"2024-03-04T16:25:19.945460Z","shell.execute_reply.started":"2024-03-04T16:25:19.558961Z","shell.execute_reply":"2024-03-04T16:25:19.944506Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"time taken: 0.38s\nShape of x: (5000, 250), shape of y: (5000,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Custom tensor dataset class","metadata":{}},{"cell_type":"code","source":"# Turn on GPU before processing further\nis_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\") if is_cuda else torch.device(\"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:25:19.946564Z","iopub.execute_input":"2024-03-04T16:25:19.946851Z","iopub.status.idle":"2024-03-04T16:25:20.016591Z","shell.execute_reply.started":"2024-03-04T16:25:19.946827Z","shell.execute_reply":"2024-03-04T16:25:20.015754Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 50\ntrain_dataset = TensorDataset(torch.from_numpy(train_x), torch.Tensor(train_y))\ntrain_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(seed), drop_last=True)\n\nvalidate_dataset = TensorDataset(torch.from_numpy(validate_x), torch.from_numpy(validate_y))\nvalidate_data_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(seed), drop_last=True)\n\ntest_dataset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\ntest_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(seed), drop_last=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:25:20.017827Z","iopub.execute_input":"2024-03-04T16:25:20.018183Z","iopub.status.idle":"2024-03-04T16:25:20.120622Z","shell.execute_reply.started":"2024-03-04T16:25:20.018155Z","shell.execute_reply":"2024-03-04T16:25:20.119664Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_data_loader)\nsample_x, sample_y = next(dataiter)\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint()\nprint('Sample label size: ', sample_y.size()) # batch_size\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:25:20.121737Z","iopub.execute_input":"2024-03-04T16:25:20.122315Z","iopub.status.idle":"2024-03-04T16:25:20.215360Z","shell.execute_reply.started":"2024-03-04T16:25:20.122289Z","shell.execute_reply":"2024-03-04T16:25:20.214492Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Sample input size:  torch.Size([50, 250])\n\nSample label size:  torch.Size([50])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train model","metadata":{}},{"cell_type":"code","source":"# Using Early stopper to stop when the F1Score prediction drops \nclass EarlyStopper:\n    def __init__(self, patience=3, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.min_f1_score_ = 0\n        self.min_accuracy = 0\n\n    def early_stop_f1(self, f1_score):\n        if f1_score_ > self.min_f1_score_:\n            self.min_f1_score_ = f1_score\n            self.counter = 0\n        elif f1_score < (self.min_f1_score_ + self.min_delta):\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False\n\n    def early_stop_accuracy(self, accuracy):\n        if accuracy > self.min_accuracy:\n            self.min_accuracy = accuracy\n            self.counter = 0\n        elif accuracy < (self.min_accuracy + self.min_delta):\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:25:20.216530Z","iopub.execute_input":"2024-03-04T16:25:20.217091Z","iopub.status.idle":"2024-03-04T16:25:20.225283Z","shell.execute_reply.started":"2024-03-04T16:25:20.217058Z","shell.execute_reply":"2024-03-04T16:25:20.224213Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n \nclass SentimentalLSTM(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):    \n        \"\"\"\n        Initialize the model by setting up the layers\n        \"\"\"\n        super().__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # Embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        # Dropout layer\n        self.dropout=nn.Dropout(p=drop_prob)\n        \n        # Linear and sigmoid layer\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n        \n        # Embadding and LSTM output\n        embedd = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedd, hidden)\n        \n        # Stack up the lstm output\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # Dropout and fully connected layers\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        sig_out = self.sigmoid(out)\n        \n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        \"\"\"Initialize Hidden STATE\"\"\"\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (is_cuda):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden\n    \ndef save_model(model, name=\"model.pth\"):\n    torch.save(model.state_dict(), name)\n    return True\n\ndef load_model(model, name=\"model.pth\"):\n    loaded_model.load_state_dict(torch.load('model.pth'))\n    return True","metadata":{"execution":{"iopub.status.busy":"2024-03-04T16:25:20.226495Z","iopub.execute_input":"2024-03-04T16:25:20.226775Z","iopub.status.idle":"2024-03-04T16:25:20.244595Z","shell.execute_reply.started":"2024-03-04T16:25:20.226747Z","shell.execute_reply":"2024-03-04T16:25:20.243803Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# # Helper functions to train and test model\n# def train_model(model, train_data_loader, validation_data_loader, device, clip, epochs=10, steps=0, print_step=100,\n#                 learning_rate=0.001, weights=None, early_stopper=None, debug=True):\n#     \"\"\"\n#     Trains the model.\n#     Returns:\n#         train_accuracies: list[float], contains the training accuracy for each epoch\n#         validation_accuracies: list[float], contains the validation accuracy for each epoch\n#         train_losses: list[float], contains the training loss for each epoch\n#         validation_losses: list[float], contains the validation loss for each epoch\n#         time_taken: float, the time taken for the model to run finish\n#     \"\"\"\n#     # Lists to store train and test accuracies and losses\n#     train_accuracies, validation_accuracies = [], []\n#     train_losses, validation_losses = [], []\n\n#     # Define loss function and optimizer\n#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n#     loss_fn = nn.BCELoss()\n#     h = model.init_hidden(batch_size)\n#     model = model.to(device)\n\n#     # Record the start time\n#     start_time = time.time()\n    \n#     # Training loop\n#     for epoch in tqdm.tqdm(range(epochs)):\n#         # =======================Training=======================\n#         model.train()\n        \n#         train_loss = 0.0\n#         total_train, correct_train = 0, 0\n        \n#         for inputs, labels in train_data_loader:\n#             steps += 1\n#             batch_inputs, batch_labels = inputs.to(device), labels.to(device)\n            \n#             # Creating new variables for the hidden state, otherwise\n#             # we'd backprop through the entire training history\n#             h = tuple([each.data for each in h])\n\n#             # zero accumulated gradients\n#             optimizer.zero_grad()\n\n#             # get the output from the model\n#             outputs, h = model(batch_inputs, h)\n\n#             # calculate the loss and perform backprop\n#             loss = loss_fn(outputs.squeeze(), batch_labels.float())\n# #             loss = loss_fn(outputs.squeeze(), batch_labels.float())\n#             loss.backward()\n\n#             # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n#             nn.utils.clip_grad_norm_(model.parameters(), clip)\n#             optimizer.step()\n\n#             train_loss += loss.item()\n\n#             # Calculate train accuracy\n#             predicted = torch.argmax(outputs)\n#             total_train += batch_labels.size(0)\n#             correct_train += (predicted == batch_labels).sum().item()\n#             del batch_inputs, batch_labels\n#             torch.cuda.empty_cache()\n\n#             train_accuracy = correct_train / total_train\n#             train_accuracies.append(train_accuracy)\n#             # Store train loss\n#             train_losses.append(train_loss)\n\n#             # =======================Validation=======================\n#             if steps % print_step == 0:\n#                 model.eval()\n#                 val_loss = 0.0\n#                 correct_validation, total_validation = 0, 0\n#                 val_h = model.init_hidden(batch_size)\n\n#         #         with torch.no_grad():\n#                 for inputs, labels in validation_data_loader:\n#                     # Creating new variables for the hidden state, otherwise\n#                     # we'd backprop through the entire training history\n#                     val_h = tuple([each.data for each in val_h])\n\n#                     batch_inputs, batch_labels = inputs.to(device), labels.to(device)\n\n#                     outputs, val_h = model(batch_inputs, val_h)\n\n#         #                 loss = loss_fn(outputs, batch_labels.view(-1).float())\n#                     loss = loss_fn(outputs.squeeze(), batch_labels.float())\n#                     val_loss += loss.item()\n\n#                     # Calculate test accuracy\n#                     predicted = torch.argmax(outputs)\n#                     total_validation += batch_labels.size(0)\n#                     correct_validation += (predicted == batch_labels).sum().item()\n                    \n#                     model.train()\n\n#                 validation_accuracy = correct_validation / total_validation\n#                 validation_accuracies.append(validation_accuracy)\n#                 validation_losses.append(val_loss)  # Store test loss\n\n#                 # Print validation results\n#                 if debug:\n#                     print(f\"Steps: {steps}\\nTrain loss: {train_losses[-1]:.4f}\\nValidation loss: {validation_losses[-1]:.4f}\\nTrain accuracy: {train_accuracy:.4f}\\nValidation accuracy: {validation_accuracy:.4f}\")\n\n#         # Print epoch results\n#         if debug:\n#             print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {validation_losses[-1]:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n\n#         # Check for early stopping\n#         if early_stopper and early_stopper.early_stop_accuracy(validation_accuracy):\n#             print(f\"Early stopping at epoch ({epoch+1}) due to no improvement in accuracy.\")\n#             break\n\n#     time_taken = time.time() - start_time\n#     print(f\"Time taken for the model to run finish: {time_taken:.2f} seconds\")\n#     torch.cuda.empty_cache()  # Release cache\n#     return train_accuracies, validation_accuracies, train_losses, validation_losses, time_taken\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:49:28.176267Z","iopub.execute_input":"2024-02-27T18:49:28.176646Z","iopub.status.idle":"2024-02-27T18:49:28.195233Z","shell.execute_reply.started":"2024-02-27T18:49:28.176619Z","shell.execute_reply":"2024-02-27T18:49:28.194336Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model w/ hyperparams\nvocab_size = len(vocab) + 1 # +1 for the 0 padding\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2\n\nmodel = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:49:28.346588Z","iopub.execute_input":"2024-02-27T18:49:28.346944Z","iopub.status.idle":"2024-02-27T18:49:29.094892Z","shell.execute_reply.started":"2024-02-27T18:49:28.346915Z","shell.execute_reply":"2024-02-27T18:49:29.093964Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"SentimentalLSTM(\n  (embedding): Embedding(179843, 400)\n  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(model, train_data_loader, validate_data_loader, device, lr=0.001, counter=0, epochs=50, print_every=100, clip=5, early_stopper=None):\n    model.to(device)\n    \n    start_time = time.time()\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    \n    # train for some number of epochs\n    for e in tqdm.tqdm(range(epochs)):\n        model.train()\n        # initialize hidden state\n        h = model.init_hidden(batch_size)\n\n        # ============================ Train Model ==================================\n        for inputs, labels in train_data_loader:\n            counter += 1\n\n            if(is_cuda):\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n\n            # Creating new variables for the hidden state, otherwise it will backprop through the entire training history\n            h = tuple([each.data for each in h])\n\n            # zero accumulated gradients\n            optimizer.zero_grad()\n\n            # get the output from the model\n            output, h = model(inputs, h)\n\n            # calculate the loss and perform backprop\n            loss = criterion(output.squeeze(), labels.float())\n            loss.backward()\n\n            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n            nn.utils.clip_grad_norm_(net.parameters(), clip)\n            optimizer.step()\n\n            # loss stats\n            if counter % print_every == 0:\n                # Get validation loss\n                val_h = model.init_hidden(batch_size)\n                val_losses = []\n                model.eval()\n                \n                # ============================== Validate Model ======================================\n                for inputs, labels in validate_data_loader:\n\n                    # Creating new variables for the hidden state, otherwise\n                    # we'd backprop through the entire training history\n                    val_h = tuple([each.data for each in val_h])\n\n                    inputs, labels = inputs.cuda(), labels.cuda()\n                    output, val_h = model(inputs, val_h)\n                    val_loss = criterion(output.squeeze(), labels.float())\n\n                    val_losses.append(val_loss.item())\n            \n                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.6f}...\".format(loss.item()),\n                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n                \n                validation_accuracy = correct_validation / total_validation\n\n        # Check for early stopping\n        if early_stopper and early_stopper.early_stop_accuracy(validation_accuracy):\n            print(f\"Early stopping at epoch ({epoch+1}) due to no improvement in accuracy.\")\n            break\n                \n    time_taken = time.time() - start_time\n    print(f\"Time taken for the model to run finish: {time_taken:.2f} seconds\")\n    torch.cuda.empty_cache()  # Release cache","metadata":{"execution":{"iopub.status.busy":"2024-02-27T19:12:01.049704Z","iopub.execute_input":"2024-02-27T19:12:01.050658Z","iopub.status.idle":"2024-02-27T19:12:01.065731Z","shell.execute_reply.started":"2024-02-27T19:12:01.050623Z","shell.execute_reply":"2024-02-27T19:12:01.064977Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"early_stopper = EarlyStopper(patience=5)\n\n# loss and optimization functions\nlr=0.001\n\n# training params\nepochs = 10 # 3-4 is approx where I noticed the validation loss stop decreasing\ncounter = 0\nprint_every = 100\nclip = 5 # gradient clipping\n\n# train_accuracies, validation_accuracies, train_losses, validation_losses, time_taken = train_model(model, train_data_loader,\n#                                     validation_data_loader=validate_data_loader, device=device, clip=5, debug=True)\ntrain_model(model, train_data_loader, validate_data_loader, device)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T19:12:12.022167Z","iopub.execute_input":"2024-02-27T19:12:12.022998Z","iopub.status.idle":"2024-02-27T20:06:04.302008Z","shell.execute_reply.started":"2024-02-27T19:12:12.022964Z","shell.execute_reply":"2024-02-27T20:06:04.301067Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Epoch: 1/50... Step: 100... Loss: 80.656029... Val Loss: 91.122660\nEpoch: 1/50... Step: 200... Loss: 96.354126... Val Loss: 91.155367\nEpoch: 1/50... Step: 300... Loss: 64.882690... Val Loss: 91.451450\nEpoch: 1/50... Step: 400... Loss: 96.354050... Val Loss: 91.362407\nEpoch: 1/50... Step: 500... Loss: 57.329411... Val Loss: 91.446621\nEpoch: 1/50... Step: 600... Loss: 105.961205... Val Loss: 91.088501\nEpoch: 1/50... Step: 700... Loss: 92.313782... Val Loss: 91.491643\nEpoch: 1/50... Step: 800... Loss: 89.211678... Val Loss: 91.352030\nEpoch: 2/50... Step: 900... Loss: 92.318466... Val Loss: 91.132832\nEpoch: 2/50... Step: 1000... Loss: 76.879242... Val Loss: 91.057161\nEpoch: 2/50... Step: 1100... Loss: 100.411697... Val Loss: 91.188705\nEpoch: 2/50... Step: 1200... Loss: 125.786896... Val Loss: 91.581360\nEpoch: 2/50... Step: 1300... Loss: 93.433578... Val Loss: 91.250137\nEpoch: 2/50... Step: 1400... Loss: 113.303963... Val Loss: 91.092247\nEpoch: 2/50... Step: 1500... Loss: 85.214409... Val Loss: 91.514100\nEpoch: 2/50... Step: 1600... Loss: 92.835846... Val Loss: 91.414610\nEpoch: 3/50... Step: 1700... Loss: 104.501663... Val Loss: 91.221939\nEpoch: 3/50... Step: 1800... Loss: 108.753998... Val Loss: 91.302612\nEpoch: 3/50... Step: 1900... Loss: 101.320999... Val Loss: 91.404704\nEpoch: 3/50... Step: 2000... Loss: 121.112900... Val Loss: 91.163538\nEpoch: 3/50... Step: 2100... Loss: 96.862473... Val Loss: 91.475472\nEpoch: 3/50... Step: 2200... Loss: 81.291161... Val Loss: 91.291238\nEpoch: 3/50... Step: 2300... Loss: 108.662857... Val Loss: 91.184036\nEpoch: 3/50... Step: 2400... Loss: 105.529022... Val Loss: 91.165864\nEpoch: 4/50... Step: 2500... Loss: 92.317474... Val Loss: 91.176847\nEpoch: 4/50... Step: 2600... Loss: 84.363922... Val Loss: 91.120662\nEpoch: 4/50... Step: 2700... Loss: 65.651245... Val Loss: 91.202340\nEpoch: 4/50... Step: 2800... Loss: 81.926842... Val Loss: 91.257567\nEpoch: 4/50... Step: 2900... Loss: 61.087318... Val Loss: 91.389428\nEpoch: 4/50... Step: 3000... Loss: 84.772308... Val Loss: 91.216123\nEpoch: 4/50... Step: 3100... Loss: 85.864868... Val Loss: 91.330193\nEpoch: 4/50... Step: 3200... Loss: 69.291512... Val Loss: 91.298487\nEpoch: 5/50... Step: 3300... Loss: 89.704910... Val Loss: 91.169319\nEpoch: 5/50... Step: 3400... Loss: 69.502060... Val Loss: 91.219237\nEpoch: 5/50... Step: 3500... Loss: 97.341949... Val Loss: 91.293632\nEpoch: 5/50... Step: 3600... Loss: 93.016396... Val Loss: 91.157800\nEpoch: 5/50... Step: 3700... Loss: 92.315292... Val Loss: 91.374942\nEpoch: 5/50... Step: 3800... Loss: 54.210754... Val Loss: 91.432629\nEpoch: 5/50... Step: 3900... Loss: 77.515709... Val Loss: 91.241289\nEpoch: 5/50... Step: 4000... Loss: 96.359894... Val Loss: 91.202589\nEpoch: 6/50... Step: 4100... Loss: 109.104202... Val Loss: 91.346876\nEpoch: 6/50... Step: 4200... Loss: 84.867393... Val Loss: 91.192202\nEpoch: 6/50... Step: 4300... Loss: 89.248070... Val Loss: 91.163250\nEpoch: 6/50... Step: 4400... Loss: 89.277237... Val Loss: 91.578525\nEpoch: 6/50... Step: 4500... Loss: 85.390854... Val Loss: 91.257182\nEpoch: 6/50... Step: 4600... Loss: 76.709610... Val Loss: 91.409913\nEpoch: 6/50... Step: 4700... Loss: 93.239288... Val Loss: 91.431759\nEpoch: 6/50... Step: 4800... Loss: 81.300964... Val Loss: 91.544505\nEpoch: 7/50... Step: 4900... Loss: 84.692253... Val Loss: 91.214067\nEpoch: 7/50... Step: 5000... Loss: 96.347923... Val Loss: 91.282280\nEpoch: 7/50... Step: 5100... Loss: 97.932938... Val Loss: 91.204218\nEpoch: 7/50... Step: 5200... Loss: 88.303581... Val Loss: 91.341632\nEpoch: 7/50... Step: 5300... Loss: 84.749680... Val Loss: 91.435618\nEpoch: 7/50... Step: 5400... Loss: 85.867775... Val Loss: 91.209794\nEpoch: 7/50... Step: 5500... Loss: 65.268585... Val Loss: 91.241573\nEpoch: 7/50... Step: 5600... Loss: 68.703423... Val Loss: 91.625512\nEpoch: 8/50... Step: 5700... Loss: 88.303902... Val Loss: 91.426500\nEpoch: 8/50... Step: 5800... Loss: 104.500748... Val Loss: 91.182350\nEpoch: 8/50... Step: 5900... Loss: 80.372849... Val Loss: 91.273867\nEpoch: 8/50... Step: 6000... Loss: 80.927010... Val Loss: 91.261265\nEpoch: 8/50... Step: 6100... Loss: 88.761398... Val Loss: 91.178705\nEpoch: 8/50... Step: 6200... Loss: 92.779480... Val Loss: 91.321755\nEpoch: 8/50... Step: 6300... Loss: 70.107033... Val Loss: 91.485409\nEpoch: 8/50... Step: 6400... Loss: 81.360580... Val Loss: 91.202908\nEpoch: 9/50... Step: 6500... Loss: 68.703239... Val Loss: 91.236929\nEpoch: 9/50... Step: 6600... Loss: 100.916344... Val Loss: 91.022371\nEpoch: 9/50... Step: 6700... Loss: 101.411621... Val Loss: 91.451721\nEpoch: 9/50... Step: 6800... Loss: 61.084457... Val Loss: 91.350652\nEpoch: 9/50... Step: 6900... Loss: 88.837158... Val Loss: 91.302869\nEpoch: 9/50... Step: 7000... Loss: 92.313622... Val Loss: 91.391842\nEpoch: 9/50... Step: 7100... Loss: 77.452034... Val Loss: 91.397637\nEpoch: 9/50... Step: 7200... Loss: 82.372704... Val Loss: 91.577688\nEpoch: 10/50... Step: 7300... Loss: 80.810051... Val Loss: 91.350280\nEpoch: 10/50... Step: 7400... Loss: 92.311813... Val Loss: 91.193449\nEpoch: 10/50... Step: 7500... Loss: 88.303543... Val Loss: 91.037632\nEpoch: 10/50... Step: 7600... Loss: 81.658485... Val Loss: 91.251309\nEpoch: 10/50... Step: 7700... Loss: 88.834351... Val Loss: 91.268280\nEpoch: 10/50... Step: 7800... Loss: 84.867447... Val Loss: 91.086529\nEpoch: 10/50... Step: 7900... Loss: 61.833382... Val Loss: 91.514155\nEpoch: 10/50... Step: 8000... Loss: 88.303574... Val Loss: 91.492286\nEpoch: 11/50... Step: 8100... Loss: 100.486450... Val Loss: 91.309909\nEpoch: 11/50... Step: 8200... Loss: 61.893707... Val Loss: 91.164476\nEpoch: 11/50... Step: 8300... Loss: 88.836838... Val Loss: 91.571816\nEpoch: 11/50... Step: 8400... Loss: 85.402283... Val Loss: 91.359684\nEpoch: 11/50... Step: 8500... Loss: 72.561852... Val Loss: 91.222406\nEpoch: 11/50... Step: 8600... Loss: 92.779495... Val Loss: 91.391656\nEpoch: 11/50... Step: 8700... Loss: 73.562302... Val Loss: 91.439657\nEpoch: 11/50... Step: 8800... Loss: 121.111771... Val Loss: 91.356689\nEpoch: 12/50... Step: 8900... Loss: 88.762024... Val Loss: 91.151170\nEpoch: 12/50... Step: 9000... Loss: 72.654816... Val Loss: 91.143576\nEpoch: 12/50... Step: 9100... Loss: 84.323532... Val Loss: 91.606267\nEpoch: 12/50... Step: 9200... Loss: 76.560059... Val Loss: 91.197581\nEpoch: 12/50... Step: 9300... Loss: 88.839050... Val Loss: 91.358395\nEpoch: 12/50... Step: 9400... Loss: 77.321777... Val Loss: 91.175092\nEpoch: 12/50... Step: 9500... Loss: 89.211014... Val Loss: 91.178677\nEpoch: 12/50... Step: 9600... Loss: 81.239944... Val Loss: 91.543993\nEpoch: 13/50... Step: 9700... Loss: 84.771652... Val Loss: 91.371576\nEpoch: 13/50... Step: 9800... Loss: 84.771347... Val Loss: 91.294182\nEpoch: 13/50... Step: 9900... Loss: 62.107975... Val Loss: 91.430916\nEpoch: 13/50... Step: 10000... Loss: 109.105103... Val Loss: 91.336194\nEpoch: 13/50... Step: 10100... Loss: 65.272598... Val Loss: 91.402466\nEpoch: 13/50... Step: 10200... Loss: 96.347954... Val Loss: 91.226431\nEpoch: 13/50... Step: 10300... Loss: 104.500763... Val Loss: 91.374071\nEpoch: 13/50... Step: 10400... Loss: 81.372849... Val Loss: 91.430074\nEpoch: 14/50... Step: 10500... Loss: 88.796432... Val Loss: 91.250074\nEpoch: 14/50... Step: 10600... Loss: 69.871628... Val Loss: 91.515054\nEpoch: 14/50... Step: 10700... Loss: 68.703217... Val Loss: 91.173979\nEpoch: 14/50... Step: 10800... Loss: 100.916527... Val Loss: 91.476923\nEpoch: 14/50... Step: 10900... Loss: 102.377731... Val Loss: 91.278734\nEpoch: 14/50... Step: 11000... Loss: 92.311958... Val Loss: 91.402089\nEpoch: 14/50... Step: 11100... Loss: 96.862297... Val Loss: 91.429737\nEpoch: 14/50... Step: 11200... Loss: 100.847214... Val Loss: 91.303617\nEpoch: 15/50... Step: 11300... Loss: 69.606049... Val Loss: 91.444606\nEpoch: 15/50... Step: 11400... Loss: 72.977196... Val Loss: 91.647979\nEpoch: 15/50... Step: 11500... Loss: 72.976875... Val Loss: 91.503827\nEpoch: 15/50... Step: 11600... Loss: 104.995796... Val Loss: 91.318820\nEpoch: 15/50... Step: 11700... Loss: 88.303459... Val Loss: 91.265182\nEpoch: 15/50... Step: 11800... Loss: 72.561920... Val Loss: 91.152403\nEpoch: 15/50... Step: 11900... Loss: 85.759567... Val Loss: 91.254489\nEpoch: 15/50... Step: 12000... Loss: 88.303497... Val Loss: 91.522047\nEpoch: 16/50... Step: 12100... Loss: 104.997192... Val Loss: 91.385649\nEpoch: 16/50... Step: 12200... Loss: 109.616089... Val Loss: 91.372247\nEpoch: 16/50... Step: 12300... Loss: 104.996727... Val Loss: 91.471870\nEpoch: 16/50... Step: 12400... Loss: 108.616501... Val Loss: 91.338351\nEpoch: 16/50... Step: 12500... Loss: 108.616394... Val Loss: 91.347853\nEpoch: 16/50... Step: 12600... Loss: 109.104141... Val Loss: 91.712725\nEpoch: 16/50... Step: 12700... Loss: 104.500664... Val Loss: 91.267645\nEpoch: 16/50... Step: 12800... Loss: 85.321121... Val Loss: 91.443313\nEpoch: 17/50... Step: 12900... Loss: 58.398064... Val Loss: 91.361221\nEpoch: 17/50... Step: 13000... Loss: 74.383957... Val Loss: 91.226094\nEpoch: 17/50... Step: 13100... Loss: 113.756470... Val Loss: 91.318599\nEpoch: 17/50... Step: 13200... Loss: 77.017113... Val Loss: 91.466620\nEpoch: 17/50... Step: 13300... Loss: 81.810059... Val Loss: 91.157748\nEpoch: 17/50... Step: 13400... Loss: 84.323906... Val Loss: 91.274417\nEpoch: 17/50... Step: 13500... Loss: 96.347946... Val Loss: 91.199017\nEpoch: 17/50... Step: 13600... Loss: 101.411041... Val Loss: 91.217952\nEpoch: 18/50... Step: 13700... Loss: 80.810898... Val Loss: 91.224618\nEpoch: 18/50... Step: 13800... Loss: 109.119476... Val Loss: 91.218918\nEpoch: 18/50... Step: 13900... Loss: 101.692673... Val Loss: 91.593015\nEpoch: 18/50... Step: 14000... Loss: 100.411293... Val Loss: 91.216417\nEpoch: 18/50... Step: 14100... Loss: 88.321373... Val Loss: 91.306949\nEpoch: 18/50... Step: 14200... Loss: 100.411087... Val Loss: 91.346586\nEpoch: 18/50... Step: 14300... Loss: 61.698040... Val Loss: 91.348095\nEpoch: 18/50... Step: 14400... Loss: 64.877129... Val Loss: 91.424632\nEpoch: 19/50... Step: 14500... Loss: 84.323517... Val Loss: 91.235960\nEpoch: 19/50... Step: 14600... Loss: 88.303459... Val Loss: 91.267049\nEpoch: 19/50... Step: 14700... Loss: 81.373901... Val Loss: 91.462961\nEpoch: 19/50... Step: 14800... Loss: 84.323822... Val Loss: 91.217025\nEpoch: 19/50... Step: 14900... Loss: 65.217636... Val Loss: 91.570371\nEpoch: 19/50... Step: 15000... Loss: 84.771217... Val Loss: 91.182423\nEpoch: 19/50... Step: 15100... Loss: 121.576035... Val Loss: 91.246015\nEpoch: 19/50... Step: 15200... Loss: 72.561882... Val Loss: 91.308756\nEpoch: 20/50... Step: 15300... Loss: 68.703369... Val Loss: 91.331677\nEpoch: 20/50... Step: 15400... Loss: 84.323593... Val Loss: 91.259689\nEpoch: 20/50... Step: 15500... Loss: 97.069946... Val Loss: 91.675690\nEpoch: 20/50... Step: 15600... Loss: 76.453674... Val Loss: 91.233501\nEpoch: 20/50... Step: 15700... Loss: 100.916443... Val Loss: 91.364109\nEpoch: 20/50... Step: 15800... Loss: 68.703247... Val Loss: 91.350865\nEpoch: 20/50... Step: 15900... Loss: 92.311829... Val Loss: 91.305471\nEpoch: 20/50... Step: 16000... Loss: 92.311859... Val Loss: 91.355112\nEpoch: 21/50... Step: 16100... Loss: 66.268082... Val Loss: 91.359160\nEpoch: 21/50... Step: 16200... Loss: 121.638931... Val Loss: 91.239068\nEpoch: 21/50... Step: 16300... Loss: 100.437668... Val Loss: 91.630204\nEpoch: 21/50... Step: 16400... Loss: 61.462536... Val Loss: 91.267589\nEpoch: 21/50... Step: 16500... Loss: 121.149620... Val Loss: 91.523587\nEpoch: 21/50... Step: 16600... Loss: 92.311920... Val Loss: 91.242711\nEpoch: 21/50... Step: 16700... Loss: 65.268044... Val Loss: 91.520150\nEpoch: 21/50... Step: 16800... Loss: 104.500687... Val Loss: 91.688672\nEpoch: 22/50... Step: 16900... Loss: 96.867188... Val Loss: 91.557448\nEpoch: 22/50... Step: 17000... Loss: 88.303444... Val Loss: 91.521843\nEpoch: 22/50... Step: 17100... Loss: 108.616081... Val Loss: 91.591197\nEpoch: 22/50... Step: 17200... Loss: 104.502678... Val Loss: 91.468803\nEpoch: 22/50... Step: 17300... Loss: 88.482697... Val Loss: 91.524129\nEpoch: 22/50... Step: 17400... Loss: 92.674789... Val Loss: 91.361197\nEpoch: 22/50... Step: 17500... Loss: 100.411804... Val Loss: 91.378865\nEpoch: 22/50... Step: 17600... Loss: 78.512299... Val Loss: 91.485389\nEpoch: 23/50... Step: 17700... Loss: 80.845978... Val Loss: 91.219915\nEpoch: 23/50... Step: 17800... Loss: 100.411072... Val Loss: 91.302247\nEpoch: 23/50... Step: 17900... Loss: 104.500748... Val Loss: 91.506188\nEpoch: 23/50... Step: 18000... Loss: 76.451965... Val Loss: 91.208398\nEpoch: 23/50... Step: 18100... Loss: 100.411057... Val Loss: 91.188249\nEpoch: 23/50... Step: 18200... Loss: 76.879066... Val Loss: 91.450727\nEpoch: 23/50... Step: 18300... Loss: 80.372787... Val Loss: 91.196773\nEpoch: 23/50... Step: 18400... Loss: 84.787392... Val Loss: 91.284929\nEpoch: 24/50... Step: 18500... Loss: 88.761337... Val Loss: 91.269382\nEpoch: 24/50... Step: 18600... Loss: 96.347893... Val Loss: 91.323765\nEpoch: 24/50... Step: 18700... Loss: 80.372757... Val Loss: 91.238662\nEpoch: 24/50... Step: 18800... Loss: 100.411049... Val Loss: 91.340347\nEpoch: 24/50... Step: 18900... Loss: 72.564827... Val Loss: 91.313064\nEpoch: 24/50... Step: 19000... Loss: 80.372780... Val Loss: 91.309795\nEpoch: 24/50... Step: 19100... Loss: 88.303452... Val Loss: 91.543830\nEpoch: 24/50... Step: 19200... Loss: 96.824989... Val Loss: 91.370984\nEpoch: 25/50... Step: 19300... Loss: 104.996971... Val Loss: 91.671897\nEpoch: 25/50... Step: 19400... Loss: 73.138359... Val Loss: 91.570644\nEpoch: 25/50... Step: 19500... Loss: 96.824928... Val Loss: 91.318978\nEpoch: 25/50... Step: 19600... Loss: 117.394127... Val Loss: 91.474088\nEpoch: 25/50... Step: 19700... Loss: 72.561829... Val Loss: 91.385206\nEpoch: 25/50... Step: 19800... Loss: 81.810104... Val Loss: 91.431944\nEpoch: 25/50... Step: 19900... Loss: 97.347977... Val Loss: 91.211647\nEpoch: 25/50... Step: 20000... Loss: 113.236633... Val Loss: 91.368546\nEpoch: 26/50... Step: 20100... Loss: 77.452972... Val Loss: 91.301550\nEpoch: 26/50... Step: 20200... Loss: 72.711220... Val Loss: 91.628917\nEpoch: 26/50... Step: 20300... Loss: 88.303459... Val Loss: 91.311928\nEpoch: 26/50... Step: 20400... Loss: 88.303482... Val Loss: 91.476678\nEpoch: 26/50... Step: 20500... Loss: 90.211037... Val Loss: 91.409238\nEpoch: 26/50... Step: 20600... Loss: 92.820023... Val Loss: 91.501571\nEpoch: 26/50... Step: 20700... Loss: 100.899307... Val Loss: 91.640341\nEpoch: 26/50... Step: 20800... Loss: 84.426498... Val Loss: 91.653504\nEpoch: 27/50... Step: 20900... Loss: 116.922226... Val Loss: 91.652242\nEpoch: 27/50... Step: 21000... Loss: 84.896530... Val Loss: 91.381961\nEpoch: 27/50... Step: 21100... Loss: 92.835587... Val Loss: 91.708108\nEpoch: 27/50... Step: 21200... Loss: 92.836792... Val Loss: 91.192108\nEpoch: 27/50... Step: 21300... Loss: 89.303452... Val Loss: 91.207456\nEpoch: 27/50... Step: 21400... Loss: 88.836998... Val Loss: 91.287930\nEpoch: 27/50... Step: 21500... Loss: 92.835571... Val Loss: 91.268349\nEpoch: 27/50... Step: 21600... Loss: 73.383820... Val Loss: 91.281457\nEpoch: 28/50... Step: 21700... Loss: 104.500717... Val Loss: 91.355435\nEpoch: 28/50... Step: 21800... Loss: 88.303452... Val Loss: 91.146904\nEpoch: 28/50... Step: 21900... Loss: 88.303436... Val Loss: 91.203875\nEpoch: 28/50... Step: 22000... Loss: 92.311813... Val Loss: 91.165704\nEpoch: 28/50... Step: 22100... Loss: 106.005753... Val Loss: 91.482694\nEpoch: 28/50... Step: 22200... Loss: 80.372841... Val Loss: 91.307846\nEpoch: 28/50... Step: 22300... Loss: 108.616096... Val Loss: 91.169125\nEpoch: 28/50... Step: 22400... Loss: 92.779449... Val Loss: 91.174038\nEpoch: 29/50... Step: 22500... Loss: 112.756790... Val Loss: 91.312244\nEpoch: 29/50... Step: 22600... Loss: 76.451920... Val Loss: 91.296922\nEpoch: 29/50... Step: 22700... Loss: 92.311806... Val Loss: 91.632640\nEpoch: 29/50... Step: 22800... Loss: 96.824936... Val Loss: 91.399122\nEpoch: 29/50... Step: 22900... Loss: 84.323532... Val Loss: 91.274697\nEpoch: 29/50... Step: 23000... Loss: 125.329941... Val Loss: 91.308799\nEpoch: 29/50... Step: 23100... Loss: 84.865479... Val Loss: 91.304844\nEpoch: 29/50... Step: 23200... Loss: 76.878967... Val Loss: 91.250548\nEpoch: 30/50... Step: 23300... Loss: 69.106438... Val Loss: 91.252743\nEpoch: 30/50... Step: 23400... Loss: 84.771301... Val Loss: 91.228758\nEpoch: 30/50... Step: 23500... Loss: 116.922180... Val Loss: 91.404857\nEpoch: 30/50... Step: 23600... Loss: 101.412720... Val Loss: 91.580160\nEpoch: 30/50... Step: 23700... Loss: 69.292679... Val Loss: 91.461954\nEpoch: 30/50... Step: 23800... Loss: 84.323601... Val Loss: 91.441272\nEpoch: 30/50... Step: 23900... Loss: 88.303436... Val Loss: 91.236259\nEpoch: 30/50... Step: 24000... Loss: 69.703201... Val Loss: 91.264961\nEpoch: 31/50... Step: 24100... Loss: 84.324265... Val Loss: 91.255874\nEpoch: 31/50... Step: 24200... Loss: 88.824104... Val Loss: 91.183276\nEpoch: 31/50... Step: 24300... Loss: 72.976662... Val Loss: 91.363155\nEpoch: 31/50... Step: 24400... Loss: 72.562653... Val Loss: 91.296597\nEpoch: 31/50... Step: 24500... Loss: 72.903923... Val Loss: 91.549254\nEpoch: 31/50... Step: 24600... Loss: 77.017120... Val Loss: 91.255987\nEpoch: 31/50... Step: 24700... Loss: 93.238647... Val Loss: 91.372699\nEpoch: 31/50... Step: 24800... Loss: 88.372551... Val Loss: 91.336909\nEpoch: 32/50... Step: 24900... Loss: 61.462486... Val Loss: 91.494224\nEpoch: 32/50... Step: 25000... Loss: 100.411041... Val Loss: 91.486330\nEpoch: 32/50... Step: 25100... Loss: 104.500999... Val Loss: 91.423487\nEpoch: 32/50... Step: 25200... Loss: 116.922173... Val Loss: 91.193687\nEpoch: 32/50... Step: 25300... Loss: 72.561813... Val Loss: 91.393307\nEpoch: 32/50... Step: 25400... Loss: 73.385315... Val Loss: 91.237801\nEpoch: 32/50... Step: 25500... Loss: 76.871857... Val Loss: 91.431352\nEpoch: 32/50... Step: 25600... Loss: 96.347885... Val Loss: 91.408354\nEpoch: 33/50... Step: 25700... Loss: 104.995544... Val Loss: 91.388105\nEpoch: 33/50... Step: 25800... Loss: 76.878296... Val Loss: 91.396791\nEpoch: 33/50... Step: 25900... Loss: 69.106461... Val Loss: 91.293890\nEpoch: 33/50... Step: 26000... Loss: 77.317131... Val Loss: 91.213947\nEpoch: 33/50... Step: 26100... Loss: 69.495621... Val Loss: 91.435335\nEpoch: 33/50... Step: 26200... Loss: 88.303436... Val Loss: 91.237702\nEpoch: 33/50... Step: 26300... Loss: 50.572353... Val Loss: 91.408977\nEpoch: 33/50... Step: 26400... Loss: 96.824928... Val Loss: 91.322575\nEpoch: 34/50... Step: 26500... Loss: 69.291656... Val Loss: 91.414844\nEpoch: 34/50... Step: 26600... Loss: 104.500671... Val Loss: 91.275148\nEpoch: 34/50... Step: 26700... Loss: 109.104103... Val Loss: 91.249623\nEpoch: 34/50... Step: 26800... Loss: 92.779442... Val Loss: 91.444740\nEpoch: 34/50... Step: 26900... Loss: 89.303436... Val Loss: 91.359011\nEpoch: 34/50... Step: 27000... Loss: 88.761345... Val Loss: 91.442789\nEpoch: 34/50... Step: 27100... Loss: 80.809967... Val Loss: 91.545696\nEpoch: 34/50... Step: 27200... Loss: 53.602905... Val Loss: 91.259430\nEpoch: 35/50... Step: 27300... Loss: 76.451912... Val Loss: 91.351312\nEpoch: 35/50... Step: 27400... Loss: 64.877083... Val Loss: 91.230933\nEpoch: 35/50... Step: 27500... Loss: 77.296562... Val Loss: 91.342519\nEpoch: 35/50... Step: 27600... Loss: 112.756775... Val Loss: 91.264994\nEpoch: 35/50... Step: 27700... Loss: 89.062668... Val Loss: 91.394062\nEpoch: 35/50... Step: 27800... Loss: 72.561783... Val Loss: 91.097982\nEpoch: 35/50... Step: 27900... Loss: 76.451912... Val Loss: 91.187407\nEpoch: 35/50... Step: 28000... Loss: 100.916328... Val Loss: 91.400212\nEpoch: 36/50... Step: 28100... Loss: 84.323547... Val Loss: 91.302338\nEpoch: 36/50... Step: 28200... Loss: 92.312111... Val Loss: 91.328556\nEpoch: 36/50... Step: 28300... Loss: 80.810051... Val Loss: 91.297783\nEpoch: 36/50... Step: 28400... Loss: 92.835571... Val Loss: 91.621777\nEpoch: 36/50... Step: 28500... Loss: 80.372787... Val Loss: 91.430702\nEpoch: 36/50... Step: 28600... Loss: 88.761467... Val Loss: 91.118718\nEpoch: 36/50... Step: 28700... Loss: 73.706238... Val Loss: 91.311777\nEpoch: 36/50... Step: 28800... Loss: 57.330280... Val Loss: 91.267852\nEpoch: 37/50... Step: 28900... Loss: 81.372742... Val Loss: 91.277882\nEpoch: 37/50... Step: 29000... Loss: 72.976822... Val Loss: 91.273472\nEpoch: 37/50... Step: 29100... Loss: 80.712379... Val Loss: 91.319763\nEpoch: 37/50... Step: 29200... Loss: 61.084373... Val Loss: 91.401884\nEpoch: 37/50... Step: 29300... Loss: 122.157089... Val Loss: 91.159409\nEpoch: 37/50... Step: 29400... Loss: 116.922226... Val Loss: 91.307915\nEpoch: 37/50... Step: 29500... Loss: 100.897179... Val Loss: 91.416886\nEpoch: 37/50... Step: 29600... Loss: 100.411255... Val Loss: 91.417307\nEpoch: 38/50... Step: 29700... Loss: 92.779396... Val Loss: 91.441247\nEpoch: 38/50... Step: 29800... Loss: 72.561897... Val Loss: 91.149478\nEpoch: 38/50... Step: 29900... Loss: 104.997162... Val Loss: 91.281200\nEpoch: 38/50... Step: 30000... Loss: 77.017105... Val Loss: 91.294580\nEpoch: 38/50... Step: 30100... Loss: 57.953087... Val Loss: 91.203233\nEpoch: 38/50... Step: 30200... Loss: 76.878357... Val Loss: 91.305661\nEpoch: 38/50... Step: 30300... Loss: 88.303444... Val Loss: 91.333826\nEpoch: 38/50... Step: 30400... Loss: 64.877411... Val Loss: 91.181543\nEpoch: 39/50... Step: 30500... Loss: 80.372757... Val Loss: 91.203129\nEpoch: 39/50... Step: 30600... Loss: 72.561783... Val Loss: 91.257888\nEpoch: 39/50... Step: 30700... Loss: 96.824951... Val Loss: 91.369903\nEpoch: 39/50... Step: 30800... Loss: 84.323532... Val Loss: 91.448048\nEpoch: 39/50... Step: 30900... Loss: 76.451912... Val Loss: 91.406460\nEpoch: 39/50... Step: 31000... Loss: 113.707535... Val Loss: 91.638561\nEpoch: 39/50... Step: 31100... Loss: 92.311790... Val Loss: 91.215457\nEpoch: 39/50... Step: 31200... Loss: 77.296570... Val Loss: 91.199450\nEpoch: 40/50... Step: 31300... Loss: 92.835609... Val Loss: 91.312160\nEpoch: 40/50... Step: 31400... Loss: 69.106468... Val Loss: 91.245365\nEpoch: 40/50... Step: 31500... Loss: 76.878288... Val Loss: 91.256421\nEpoch: 40/50... Step: 31600... Loss: 80.811523... Val Loss: 91.279634\nEpoch: 40/50... Step: 31700... Loss: 104.519669... Val Loss: 91.235364\nEpoch: 40/50... Step: 31800... Loss: 96.862259... Val Loss: 91.374629\nEpoch: 40/50... Step: 31900... Loss: 88.303444... Val Loss: 91.399663\nEpoch: 40/50... Step: 32000... Loss: 80.810974... Val Loss: 91.496372\nEpoch: 41/50... Step: 32100... Loss: 104.997147... Val Loss: 91.330680\nEpoch: 41/50... Step: 32200... Loss: 96.824944... Val Loss: 91.303788\nEpoch: 41/50... Step: 32300... Loss: 104.502144... Val Loss: 91.371023\nEpoch: 41/50... Step: 32400... Loss: 92.311798... Val Loss: 91.298513\nEpoch: 41/50... Step: 32500... Loss: 80.818268... Val Loss: 91.366482\nEpoch: 41/50... Step: 32600... Loss: 84.323517... Val Loss: 91.374999\nEpoch: 41/50... Step: 32700... Loss: 96.347885... Val Loss: 91.341015\nEpoch: 41/50... Step: 32800... Loss: 100.411041... Val Loss: 91.358014\nEpoch: 42/50... Step: 32900... Loss: 104.500763... Val Loss: 91.339802\nEpoch: 42/50... Step: 33000... Loss: 80.372826... Val Loss: 91.279700\nEpoch: 42/50... Step: 33100... Loss: 96.824936... Val Loss: 91.397546\nEpoch: 42/50... Step: 33200... Loss: 84.324753... Val Loss: 91.398369\nEpoch: 42/50... Step: 33300... Loss: 96.347885... Val Loss: 91.351929\nEpoch: 42/50... Step: 33400... Loss: 92.835579... Val Loss: 91.452855\nEpoch: 42/50... Step: 33500... Loss: 80.372757... Val Loss: 91.497923\nEpoch: 42/50... Step: 33600... Loss: 93.835571... Val Loss: 91.522255\nEpoch: 43/50... Step: 33700... Loss: 80.372841... Val Loss: 91.467422\nEpoch: 43/50... Step: 33800... Loss: 76.451904... Val Loss: 91.462620\nEpoch: 43/50... Step: 33900... Loss: 88.303452... Val Loss: 91.499648\nEpoch: 43/50... Step: 34000... Loss: 108.616081... Val Loss: 91.283050\nEpoch: 43/50... Step: 34100... Loss: 88.303436... Val Loss: 91.399984\nEpoch: 43/50... Step: 34200... Loss: 61.084320... Val Loss: 91.362792\nEpoch: 43/50... Step: 34300... Loss: 76.878288... Val Loss: 91.302619\nEpoch: 43/50... Step: 34400... Loss: 84.323517... Val Loss: 91.228014\nEpoch: 44/50... Step: 34500... Loss: 104.500656... Val Loss: 91.289688\nEpoch: 44/50... Step: 34600... Loss: 96.348427... Val Loss: 91.326491\nEpoch: 44/50... Step: 34700... Loss: 100.411057... Val Loss: 91.546965\nEpoch: 44/50... Step: 34800... Loss: 92.835571... Val Loss: 91.428519\nEpoch: 44/50... Step: 34900... Loss: 93.835587... Val Loss: 91.374919\nEpoch: 44/50... Step: 35000... Loss: 78.017097... Val Loss: 91.477092\nEpoch: 44/50... Step: 35100... Loss: 88.303444... Val Loss: 91.517868\nEpoch: 44/50... Step: 35200... Loss: 104.500671... Val Loss: 91.499197\nEpoch: 45/50... Step: 35300... Loss: 88.303444... Val Loss: 91.423784\nEpoch: 45/50... Step: 35400... Loss: 85.210884... Val Loss: 91.527000\nEpoch: 45/50... Step: 35500... Loss: 100.411041... Val Loss: 91.498949\nEpoch: 45/50... Step: 35600... Loss: 109.616089... Val Loss: 91.269653\nEpoch: 45/50... Step: 35700... Loss: 88.303444... Val Loss: 91.322795\nEpoch: 45/50... Step: 35800... Loss: 97.293701... Val Loss: 91.393648\nEpoch: 45/50... Step: 35900... Loss: 93.137520... Val Loss: 91.633446\nEpoch: 45/50... Step: 36000... Loss: 76.451920... Val Loss: 91.522098\nEpoch: 46/50... Step: 36100... Loss: 49.916382... Val Loss: 91.397420\nEpoch: 46/50... Step: 36200... Loss: 109.104362... Val Loss: 91.519318\nEpoch: 46/50... Step: 36300... Loss: 88.761337... Val Loss: 91.467608\nEpoch: 46/50... Step: 36400... Loss: 84.323532... Val Loss: 91.337339\nEpoch: 46/50... Step: 36500... Loss: 88.779396... Val Loss: 91.373728\nEpoch: 46/50... Step: 36600... Loss: 80.927002... Val Loss: 91.336309\nEpoch: 46/50... Step: 36700... Loss: 72.561783... Val Loss: 91.121234\nEpoch: 46/50... Step: 36800... Loss: 88.303436... Val Loss: 91.312314\nEpoch: 47/50... Step: 36900... Loss: 81.810036... Val Loss: 91.158624\nEpoch: 47/50... Step: 37000... Loss: 92.835579... Val Loss: 91.422600\nEpoch: 47/50... Step: 37100... Loss: 104.513107... Val Loss: 91.368686\nEpoch: 47/50... Step: 37200... Loss: 88.303436... Val Loss: 91.583484\nEpoch: 47/50... Step: 37300... Loss: 81.239166... Val Loss: 91.307443\nEpoch: 47/50... Step: 37400... Loss: 77.451912... Val Loss: 91.231267\nEpoch: 47/50... Step: 37500... Loss: 92.779457... Val Loss: 91.227875\nEpoch: 47/50... Step: 37600... Loss: 96.824936... Val Loss: 91.273643\nEpoch: 48/50... Step: 37700... Loss: 76.451920... Val Loss: 91.405106\nEpoch: 48/50... Step: 37800... Loss: 104.997101... Val Loss: 91.333841\nEpoch: 48/50... Step: 37900... Loss: 100.886658... Val Loss: 91.548974\nEpoch: 48/50... Step: 38000... Loss: 88.303444... Val Loss: 91.237322\nEpoch: 48/50... Step: 38100... Loss: 69.871490... Val Loss: 91.490679\nEpoch: 48/50... Step: 38200... Loss: 72.976830... Val Loss: 91.272073\nEpoch: 48/50... Step: 38300... Loss: 100.411041... Val Loss: 91.228835\nEpoch: 48/50... Step: 38400... Loss: 77.296570... Val Loss: 91.184365\nEpoch: 49/50... Step: 38500... Loss: 84.771309... Val Loss: 91.260311\nEpoch: 49/50... Step: 38600... Loss: 84.771332... Val Loss: 91.552110\nEpoch: 49/50... Step: 38700... Loss: 76.451912... Val Loss: 91.516056\nEpoch: 49/50... Step: 38800... Loss: 69.106461... Val Loss: 91.406604\nEpoch: 49/50... Step: 38900... Loss: 100.411041... Val Loss: 91.393179\nEpoch: 49/50... Step: 39000... Loss: 80.372765... Val Loss: 91.518982\nEpoch: 49/50... Step: 39100... Loss: 61.085518... Val Loss: 91.723419\nEpoch: 49/50... Step: 39200... Loss: 69.106461... Val Loss: 91.208167\nEpoch: 50/50... Step: 39300... Loss: 100.897179... Val Loss: 91.265458\nEpoch: 50/50... Step: 39400... Loss: 80.372757... Val Loss: 91.184523\nEpoch: 50/50... Step: 39500... Loss: 88.303436... Val Loss: 91.550857\nEpoch: 50/50... Step: 39600... Loss: 92.835571... Val Loss: 91.312348\nEpoch: 50/50... Step: 39700... Loss: 61.462513... Val Loss: 91.301287\nEpoch: 50/50... Step: 39800... Loss: 80.372757... Val Loss: 91.196591\nEpoch: 50/50... Step: 39900... Loss: 92.835609... Val Loss: 91.527394\nEpoch: 50/50... Step: 40000... Loss: 84.323524... Val Loss: 91.354862\n","output_type":"stream"}]},{"cell_type":"code","source":"def test_model(model, test_data_loader, device):\n    \"\"\"\n    Tests the model.\n    Returns:\n        predicted_labels: list[int], contains the predicted labels for the test data\n        actual_labels: list[int], contains the actual labels for the test data\n        test_accuracy: float, the test accuracy\n    \"\"\"\n    model = model.to(device)\n    # Initialise hidden state\n    h = model.init_hidden(batch_size)\n    model.eval()\n    \n    # Variables to track accuracy\n    correct_test, total_test = 0, 0\n    \n    # Lists to store predicted and actual labels\n    predicted_labels = []\n    actual_labels = []\n\n    # Record the start time\n    start_time = time.time()\n    # =======================Testing=======================\n    with torch.no_grad():\n        for inputs, labels in test_data_loader:\n            batch_inputs, batch_labels = inputs.to(device), labels.to(device)\n\n            outputs, h = model(batch_inputs, h)\n\n            # Calculate test accuracy\n            predicted = torch.argmax(outputs)\n            total_test += batch_labels.size(0)\n            correct_test += (predicted == batch_labels).sum().item()\n\n            # Append predicted and actual labels to the lists\n            predicted_labels.extend([predicted.tolist()])\n            actual_labels.extend(batch_labels.tolist())\n\n    time_taken = time.time() - start_time\n\n    test_accuracy = correct_test / total_test\n    print(f\"Time taken to run finish the test: {time_taken:.2f} seconds\")\n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n    torch.cuda.empty_cache()  # Release cache\n    return predicted_labels, actual_labels, test_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:19:50.048309Z","iopub.execute_input":"2024-02-26T20:19:50.048638Z","iopub.status.idle":"2024-02-26T20:19:50.058015Z","shell.execute_reply.started":"2024-02-26T20:19:50.048614Z","shell.execute_reply":"2024-02-26T20:19:50.057095Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"predicted_labels, actual_labels, test_accuracy = test_model(model, test_data_loader=test_data_loader, device=device)\nactual_labels","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:20:00.357998Z","iopub.execute_input":"2024-02-26T20:20:00.358448Z","iopub.status.idle":"2024-02-26T20:20:02.233954Z","shell.execute_reply.started":"2024-02-26T20:20:00.358412Z","shell.execute_reply":"2024-02-26T20:20:02.233108Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Time taken to run finish the test: 1.85 seconds\nTest Accuracy: 0.0390\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"[1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n ...]"},"metadata":{}}]},{"cell_type":"code","source":"# Get test data loss and accuracy\ntest_losses = [] # track loss\nnum_correct = 0\n\n# init hidden state\nh = net.init_hidden(batch_size)\n\nnet.eval()\n# iterate over test data\nfor inputs, labels in test_data_loader:\n\n    # Creating new variables for the hidden state, otherwise\n    # we'd backprop through the entire training history\n    h = tuple([each.data for each in h])\n\n    if(is_cuda):\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    \n    output, h = net(inputs, h)\n    \n    # calculate loss\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n    \n    # compare predictions to true label\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not is_cuda else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n\n# avg test loss\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n# accuracy over all test data\ntest_acc = num_correct/len(test_data_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","metadata":{"execution":{"iopub.status.busy":"2024-02-27T16:52:19.779588Z","iopub.execute_input":"2024-02-27T16:52:19.780424Z","iopub.status.idle":"2024-02-27T16:52:21.660576Z","shell.execute_reply.started":"2024-02-27T16:52:19.780390Z","shell.execute_reply":"2024-02-27T16:52:21.659711Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Test loss: 94.526\nTest accuracy: 0.861\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:18:34.160234Z","iopub.status.idle":"2024-02-25T14:18:34.160560Z","shell.execute_reply.started":"2024-02-25T14:18:34.160405Z","shell.execute_reply":"2024-02-25T14:18:34.160418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:18:34.161909Z","iopub.status.idle":"2024-02-25T14:18:34.162227Z","shell.execute_reply.started":"2024-02-25T14:18:34.162073Z","shell.execute_reply":"2024-02-25T14:18:34.162086Z"},"trusted":true},"execution_count":null,"outputs":[]}]}